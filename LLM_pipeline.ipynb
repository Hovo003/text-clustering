{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "279ce365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, fowlkes_mallows_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a02e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEYS = {\n",
    "    \"mistral\": \"daTaONGS6GBzuX9b2OqgqZ41vtsA2GTu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "635e3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = {\n",
    "    \"mistral\": \"mistral-medium\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6c1517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done.\n",
      "LLM input saved to → outputs/questions_only.csv\n",
      "Label reference saved to → outputs/labels_for_evaluation.json\n"
     ]
    }
   ],
   "source": [
    "INITIAL_INPUT = \"data/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\"\n",
    "INPUT_CSV = \"outputs/questions_only.csv\"\n",
    "LABELS_JSON = \"outputs/labels_for_evaluation.json\"\n",
    "\n",
    "# Ensure outputs directory exists\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(INITIAL_INPUT)\n",
    "\n",
    "# 1. Save LLM input: only instruction column\n",
    "questions_only = df[[\"instruction\"]].dropna()\n",
    "questions_only.to_csv(INPUT_CSV, index=False)\n",
    "\n",
    "# 2. Save label reference: instruction + intent + category\n",
    "label_reference = df[[\"instruction\", \"intent\", \"category\"]].dropna()\n",
    "label_reference.to_json(LABELS_JSON, orient=\"records\", indent=2, force_ascii=False)\n",
    "\n",
    "# Confirmation\n",
    "print(\"✅ Done.\")\n",
    "print(f\"LLM input saved to → {INPUT_CSV}\")\n",
    "print(f\"Label reference saved to → {LABELS_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d42f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_INPUT = \"data/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\"\n",
    "INPUT_CSV = \"outputs/questions_only.csv\"\n",
    "PROMPT_DIR = \"prompts\"\n",
    "BATCH_SIZE = 1350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d11843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_JSONS = {\n",
    "    \"mistral\": [\n",
    "        \"outputs/Mistral_prompt1_final_cluster_summary.json\",\n",
    "        \"outputs/Mistral_prompt2_final_cluster_summary.json\",\n",
    "        \"outputs/Mistral_prompt3_final_cluster_summary.json\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14161ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prompt(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1a928c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cluster_response(text):\n",
    "    pattern = r\"\\d+\\.\\s*(.*?)\\n(.*?)\\nCount:\\s*(\\d+)\"\n",
    "    matches = re.findall(pattern, text.strip())\n",
    "    parsed = []\n",
    "    for name, description, count in matches:\n",
    "        desc_clean = re.sub(r\"^(Description of the group:|Description:)\\s*\", \"\", description.strip())\n",
    "        parsed.append({\n",
    "            \"name\": name.strip(),\n",
    "            \"description\": desc_clean,\n",
    "            \"count\": int(count)\n",
    "        })\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f214801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_mistral(prompt, api_key):\n",
    "    url = \"https://api.mistral.ai/v1/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAMES[\"mistral\"],\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    r = requests.post(url, headers=headers, json=payload)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a48a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(INPUT_CSV)\n",
    "questions = df[\"instruction\"].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9192cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALL_FUNCTIONS = {\n",
    "    \"mistral\": call_mistral\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b74241b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 MISTRAL | Prompt 1 | Batch 1\n",
      "🔄 MISTRAL | Prompt 1 | Batch 2\n",
      "🔄 MISTRAL | Prompt 1 | Batch 3\n",
      "🔄 MISTRAL | Prompt 1 | Batch 4\n",
      "🔄 MISTRAL | Prompt 1 | Batch 5\n",
      "🔄 MISTRAL | Prompt 1 | Batch 6\n",
      "🔄 MISTRAL | Prompt 1 | Batch 7\n",
      "🔄 MISTRAL | Prompt 1 | Batch 8\n",
      "🔄 MISTRAL | Prompt 1 | Batch 9\n",
      "🔄 MISTRAL | Prompt 1 | Batch 10\n",
      "🔄 MISTRAL | Prompt 1 | Batch 11\n",
      "🔄 MISTRAL | Prompt 1 | Batch 12\n",
      "🔄 MISTRAL | Prompt 1 | Batch 13\n",
      "🔄 MISTRAL | Prompt 1 | Batch 14\n",
      "🔄 MISTRAL | Prompt 1 | Batch 15\n",
      "🔄 MISTRAL | Prompt 1 | Batch 16\n",
      "🔄 MISTRAL | Prompt 1 | Batch 17\n",
      "🔄 MISTRAL | Prompt 1 | Batch 18\n",
      "🔄 MISTRAL | Prompt 1 | Batch 19\n",
      "🔄 MISTRAL | Prompt 1 | Batch 20\n",
      "✅ Saved: outputs/Mistral_prompt1_final_cluster_summary.json\n",
      "📄 Saved: outputs/Mistral_prompt1_final_cluster_summary.csv\n",
      "🔄 MISTRAL | Prompt 2 | Batch 1\n",
      "🔄 MISTRAL | Prompt 2 | Batch 2\n",
      "🔄 MISTRAL | Prompt 2 | Batch 3\n",
      "🔄 MISTRAL | Prompt 2 | Batch 4\n",
      "🔄 MISTRAL | Prompt 2 | Batch 5\n",
      "🔄 MISTRAL | Prompt 2 | Batch 6\n",
      "🔄 MISTRAL | Prompt 2 | Batch 7\n",
      "🔄 MISTRAL | Prompt 2 | Batch 8\n",
      "🔄 MISTRAL | Prompt 2 | Batch 9\n",
      "🔄 MISTRAL | Prompt 2 | Batch 10\n",
      "🔄 MISTRAL | Prompt 2 | Batch 11\n",
      "🔄 MISTRAL | Prompt 2 | Batch 12\n",
      "🔄 MISTRAL | Prompt 2 | Batch 13\n",
      "🔄 MISTRAL | Prompt 2 | Batch 14\n",
      "🔄 MISTRAL | Prompt 2 | Batch 15\n",
      "🔄 MISTRAL | Prompt 2 | Batch 16\n",
      "🔄 MISTRAL | Prompt 2 | Batch 17\n",
      "🔄 MISTRAL | Prompt 2 | Batch 18\n",
      "🔄 MISTRAL | Prompt 2 | Batch 19\n",
      "🔄 MISTRAL | Prompt 2 | Batch 20\n",
      "✅ Saved: outputs/Mistral_prompt2_final_cluster_summary.json\n",
      "📄 Saved: outputs/Mistral_prompt2_final_cluster_summary.csv\n",
      "🔄 MISTRAL | Prompt 3 | Batch 1\n",
      "🔄 MISTRAL | Prompt 3 | Batch 2\n",
      "🔄 MISTRAL | Prompt 3 | Batch 3\n",
      "🔄 MISTRAL | Prompt 3 | Batch 4\n",
      "🔄 MISTRAL | Prompt 3 | Batch 5\n",
      "🔄 MISTRAL | Prompt 3 | Batch 6\n",
      "🔄 MISTRAL | Prompt 3 | Batch 7\n",
      "🔄 MISTRAL | Prompt 3 | Batch 8\n",
      "🔄 MISTRAL | Prompt 3 | Batch 9\n",
      "🔄 MISTRAL | Prompt 3 | Batch 10\n",
      "🔄 MISTRAL | Prompt 3 | Batch 11\n",
      "🔄 MISTRAL | Prompt 3 | Batch 12\n",
      "🔄 MISTRAL | Prompt 3 | Batch 13\n",
      "🔄 MISTRAL | Prompt 3 | Batch 14\n",
      "🔄 MISTRAL | Prompt 3 | Batch 15\n",
      "🔄 MISTRAL | Prompt 3 | Batch 16\n",
      "🔄 MISTRAL | Prompt 3 | Batch 17\n",
      "🔄 MISTRAL | Prompt 3 | Batch 18\n",
      "🔄 MISTRAL | Prompt 3 | Batch 19\n",
      "🔄 MISTRAL | Prompt 3 | Batch 20\n",
      "✅ Saved: outputs/Mistral_prompt3_final_cluster_summary.json\n",
      "📄 Saved: outputs/Mistral_prompt3_final_cluster_summary.csv\n"
     ]
    }
   ],
   "source": [
    "for model in [\"mistral\"]:\n",
    "    for prompt_index in range(3):\n",
    "        prompt_path = os.path.join(PROMPT_DIR, f\"prompt{prompt_index+1}.txt\")\n",
    "        prompt_template = read_prompt(prompt_path)\n",
    "        output_path_json = OUTPUT_JSONS[model][prompt_index]\n",
    "        output_path_csv = output_path_json.replace(\".json\", \".csv\")\n",
    "\n",
    "        merged_clusters = {}\n",
    "        assignment_rows = []\n",
    "\n",
    "        for i in range(0, len(questions), BATCH_SIZE):\n",
    "            batch = questions[i:i+BATCH_SIZE]\n",
    "            question_block = \"\\n\".join(f\"{j+1}. {q}\" for j, q in enumerate(batch))\n",
    "            prompt = prompt_template.replace(\"{{QUESTIONS}}\", question_block)\n",
    "\n",
    "            print(f\"🔄 {model.upper()} | Prompt {prompt_index+1} | Batch {i//BATCH_SIZE + 1}\")\n",
    "            try:\n",
    "                response = CALL_FUNCTIONS[model](prompt, API_KEYS[model])\n",
    "                parsed = parse_cluster_response(response)\n",
    "\n",
    "                cluster_pointer = 0\n",
    "                for cluster in parsed:\n",
    "                    name = cluster[\"name\"]\n",
    "                    desc = cluster[\"description\"]\n",
    "                    count = cluster[\"count\"]\n",
    "\n",
    "                    if name in merged_clusters:\n",
    "                        merged_clusters[name][\"count\"] += count\n",
    "                    else:\n",
    "                        merged_clusters[name] = cluster\n",
    "\n",
    "                    for _ in range(count):\n",
    "                        if cluster_pointer >= len(batch):\n",
    "                            break\n",
    "                        assignment_rows.append({\n",
    "                            \"instruction\": batch[cluster_pointer],\n",
    "                            \"name\": name,\n",
    "                            \"description\": desc\n",
    "                        })\n",
    "                        cluster_pointer += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {model.upper()} Prompt {prompt_index+1} Batch {i//BATCH_SIZE + 1} failed: {e}\")\n",
    "            sleep(1)\n",
    "\n",
    "        # Save final cluster summary (JSON)\n",
    "        with open(output_path_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(list(merged_clusters.values()), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # Save cluster assignments (CSV)\n",
    "        pd.DataFrame(assignment_rows).to_csv(output_path_csv, index=False)\n",
    "\n",
    "        print(f\"✅ Saved: {output_path_json}\")\n",
    "        print(f\"📄 Saved: {output_path_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a30d9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_paths = [\n",
    "    \"outputs/Mistral_prompt1_final_cluster_summary.csv\",\n",
    "    \"outputs/Mistral_prompt2_final_cluster_summary.csv\",\n",
    "    \"outputs/Mistral_prompt3_final_cluster_summary.csv\"\n",
    "]\n",
    "\n",
    "true_data_path = \"data/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9bd21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = pd.read_csv(true_data_path)\n",
    "true_df = true_df.dropna(subset=[\"instruction\", \"intent\"])\n",
    "true_df = true_df[[\"instruction\", \"intent\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48804283",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, path in enumerate(prompt_paths, start=1):\n",
    "    try:\n",
    "        pred_df = pd.read_csv(path)\n",
    "        pred_df = pred_df.dropna(subset=[\"instruction\", \"name\"])\n",
    "        pred_df = pred_df[[\"instruction\", \"name\"]].drop_duplicates()\n",
    "\n",
    "        # Merge on instruction\n",
    "        merged = pd.merge(true_df, pred_df, on=\"instruction\", how=\"inner\")\n",
    "        merged.columns = [\"instruction\", \"true_label\", \"predicted_label\"]\n",
    "\n",
    "        # Encode as integers (just for grouping)\n",
    "        le_true = LabelEncoder()\n",
    "        le_pred = LabelEncoder()\n",
    "\n",
    "        cluster_true = le_true.fit_transform(merged[\"true_label\"])\n",
    "        cluster_pred = le_pred.fit_transform(merged[\"predicted_label\"])\n",
    "\n",
    "        # Calculate metrics\n",
    "        ari = adjusted_rand_score(cluster_true, cluster_pred)\n",
    "        nmi = normalized_mutual_info_score(cluster_true, cluster_pred)\n",
    "        fmi = fowlkes_mallows_score(cluster_true, cluster_pred)\n",
    "\n",
    "        results.append({\n",
    "            \"Prompt\": f\"Prompt {i}\",\n",
    "            \"Sample Size\": len(merged),\n",
    "            \"ARI\": ari,\n",
    "            \"NMI\": nmi,\n",
    "            \"FMI\": fmi\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"Prompt\": f\"Prompt {i}\",\n",
    "            \"Sample Size\": 0,\n",
    "            \"ARI\": None,\n",
    "            \"NMI\": None,\n",
    "            \"FMI\": None,\n",
    "            \"Error\": str(e)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "906c64eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Prompt  Sample Size       ARI       NMI       FMI\n",
      "0  Prompt 1         9990  0.608613  0.862463  0.630986\n",
      "1  Prompt 2        10413  0.575001  0.852295  0.600736\n",
      "2  Prompt 3         9384  0.493308  0.835502  0.550257\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
